% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}


\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4}

% ISBN
%\isbn{123-4567-24-567/08/06}

%Conference
%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}

%
% --- Author Metadata here ---
\conferenceinfo{CS Text Mining}{Spring '16}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{The Automated Script Reviewer}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Katherine Schinkel\\
       \affaddr{University of Virginia}\\
       \affaddr{Data Science Institute}\\
       \email{kms6bn@virginia.edu}
% 2nd. author
\alignauthor
Marcus Rosti\\
       \affaddr{University of Virginia}\\
       \affaddr{Data Science Institute}\\
       \email{mer3ef@virginia.edu}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\date{6 May 2014}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
This paper discusses the application of text mining techniques and regression analysis to two 90s sitcoms as a means to predict the shows' viewership and rating. We collected ratings from IMDb and viewership from Wikipedia's reported Nielsen ratings. We apply TF-IDF and Latent Dirichlet Allocation to a vector space model of sitcom scripts, utilizing the results as predictors to our regression model.
% TODO this needs more information 
\end{abstract}

%
% End generated code
%

\keywords{Text Mining; Regression Analysis; Machine Learning; Automated Systems}

\section{Introduction}
Producing a television show is an enormously expensive exercise. For example, according to Priceonomics, a pilot conservatively costs \$2 million to develop a typically 30 minute comedy or over \$5 million to produce an hour long drama.\footnote{http://priceonomics.com/the-economics-of-a-hit-tv-show/} That's a substantial investment into a project that may never be seen by the public. Additionally, 98\% of scripts fail to be made into pilots and of those, only about half will lead to a full season.

This creative and expensive process touches numerous individuals, from pitching ideas for shows to actual filming the season. We will focus on the inefficient script screening and development process. Typically, a production company will field many ideas and make a call for scripts based on those ideas. A producer must then screen through scripts and manually rate them using only their intuition, which could be swayed by cognitive bias towards a particular writer. In addition to the valuable time a producer wastes reading poor scripts, the chosen script must maximize the chance of developing into a hit show. 

Our research addresses automating the script review process by providing assistance to the reviewer via an automated system. We apply text preprocessing and feature engineering to predict the continuous values of IMDb ratings and Nielsen viewership ratings. We chose these two values as our prediction value because a producer primarily cares about two things when developing a show: the show should be high quality and widely viewed. To measure high quality, the script needs to be well written and should receive a high IMDb value when it airs. To measure successful viewership, the show needs to be viewed by many millions people to drive advertising (or generate more subscriptions in the case of subscription services like HBO and Netflix). For our model to be successful, it will need to be able to predict these values with substantial certainty.

We used two 90s Sitcoms, both being widely viewed and rated, to train and test our models. Given that these shows have proliferated American culture, we expect to see some predictive power to our approach and conclude that there is a correlation between the script and the audience response.

\section{Literature Review} % Maybe this could just be done in the introduction

Researchers have explored sentiment analysis of text thoroughly. Bo Pang and Lillian Lee explored a multiclass approach to categorizing movie reviews as either positive, average or negative. They used support vector machines and a one versus all separation in order to classify each review \cite{Pang:2005:SSE:1219840.1219855}. That research expanded a traditional binary classification into a multivalued problem. 

In a separate field, Saxena explored applying data mining techniques to screen resumes as part of a job application. In this case, Saxena explains that as the internet becomes more popular, people are able to apply to many more jobs, flooding online job postings with resumes. To parse through these resumes, companies can apply several information retrieval techniques. Saxena focuses on skills extraction, finding unique features and dealing with the specific setting of the resume. They experienced lackluster results and concluded that more research would be required to create a more confident parser \cite{Charul}.

Lastly, Pivotal, a data sciences research company, formulated a script reviewer with success. They extracted information from television scripts which included speaker and scene delegation and applied extensive feature engineering. For example, they were able to determine personalities of speakers via topic modeling and include these variables in their ElasticNet regression model \cite{Pivotal}. However, the authors did not reveal details of their modeling and approach since it was a proprietary project for a television producing client.  

\hfill \break
\section{Method Overview}
Our model follows a standard pipelined approach and employs 10 fold cross validation to validate results.

\subsection{Data Parsing}
To clean the text, we follow a standard text mining pipeline of stemming, tokenizing and removing stop words. This process removes any words that are too frequent to be meaningful in predicting the outcome of an episode. Stemming combines words of different tenses and uses to further decrease the vector space model.

The two response variables are the IMDb mean rating for the episode and the Nielsen rating for viewership rating. For the IMDb rating, we paired each episode with its corresponding rating. The rating of the episode should stand on its own since it could come days weeks or years after the actual episode airs. Our claim here is that the rating is independent of the timing of the episodes air. However, for the Nielsen rating, it would not make sense to pair the episodes viewership with the actual episode. Consider someone watching the show. That person has already come to the decision to watch the show independent of the script since it is unfolding before them. We, instead, map the following episode's viewership with that episode; moreover, Script$_i$ corresponds to Viewership$_{i+1}$. We chose this approach assuming that if a person were to watch an episode and enjoy the script, then they may be more likely to watch the next episode. In this case, we captured the temporal aspect of the airing.

\subsection{Regression Inputs}
We explored two options as predictive inputs to our regression model: a TF-IDF vector space model and a topic model.
\subsubsection{TF-IDF}
Term Frequency - Inverse Document Frequency (TF-IDF) is a method of weighting tokens in a language model so as to highlight import words. It weights the number of times a word appears in a document (the Term Frequency) by the inverse of the number of documents that word appears in all documents in the training set. So for example, a stop word like 'a' will appear in every single document. A typical IDF metric will force that value to result in a 0 thus removing the term from our model. 

Our input matrix to the regression equation is a document term matrix where the values in each cell are that weighted TF-IDF value.

\subsubsection{Topic Modeling}
Latent Dirichlet Allocation (LDA) is a generative language model used to formulate topics from a document corpus. For each document in the corpus, the LDA model provides a vector of probabilities representing how likely each topic applies to that document. This allows each document to contain multiple topics, mirroring the complexity of a single sitcom episode.

\subsection{Model Validation}
We followed a testing procedure based on 10 fold cross validation. In this procedure we compared our regression model, built on each training fold, with the mean of the response, again on the training fold. We use our model to predict the response on the test set and compare it to the mean of the response on the training set. Then we employ a two sample t-test to determine whether our model exceeds the performance of the mean.

\begin{equation}
t^* = \frac{\bar{Y}_1 - \bar{Y}_2}{\sqrt{s_1^2/N_1 + s_2^2/N_2}}
\end{equation}

\section{Results}
Our null hypothesis was that the model performed just as well as the mean and our alternate hypothesis is that the model outperforms the mean in terms of mean squared error. 
\begin{center}
\begin{tabular}{|r | r || r | r | r|} \hline
% IMDb mean 0.134 mode 0.175 -1.713
% Nielsen mean 50.018 model 70.269 -1.226
% IMDb mean 0.133 0.132 0.062
% Nieslen mean 45.858 model 44.872 0.096
Method & Target & Mean & Model & t value \\ \hline\hline
 TF-IDF & IMDb & 0.134 & 0.175 & -1.713 \\ \hline
 TF-IDF & Nielsen & 50.018 & 70.269 & -1.226 \\ \hline \hline
LDA & IMDb & 0.133 & 0.132 & 0.062 \\ \hline
LDA & Nielsen & 45.858 & 44.872 & 0.096 \\ \hline
\end{tabular}
\end{center}

\section{Analysis}

Our model did not outperform the mean in any of the four tests by a statistical margin. Our TF-IDF model underperformed the mean. Our topic modeling did outperform the mean by a small margin but so small that the t-value is nearly zero. This indicates that a model based on the topics of the episode and the words used in the episode, is not more effective than a naive approach of guessing the viewership and rating by mean. Since the results did not show a stronger correlation, we do not recommend using only these values as predictors to a regression model.

\section{Future Work}

Although our modeling results indicate that there may not be a relationship between television scripts and ratings, applying more extensive feature engineering could supplement our model. Our modeling was limited to unigrams from transcripts. Future work could explore incorporating bigrams or trigrams to capture important phrases within episodes. Additionally, Pivotal achieved success with extracting metadata from the transcripts, such as count of scenes in an episode and count of characters in a scene. Pivotal also extracted speaker characteristics and indicated that they were significant to their model.

Our model was trained on a small sample size of around 250 sitcom scripts. In the future, we would like supplement our model with a larger training set of documents. Training an LDA model with a larger dataset could result in more meaningful topics that could be significant to the linear model.

Future work could also include exploring other modeling techniques. Pivotal implemented an ElasticNet regression model to significant variables (indicated by a linear model applied to each variable). Alternatively, we could explore this topic as a classification problem by binning episodes into categories. In this case, we could apply SVM, a method that proved to be successful by Pang and Lee when classifying multiclass ratings \cite{Pang:2005:SSE:1219840.1219855}.\hfill \break
\hfill \break

\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case

\balancecolumns % GM June 2007
% That's all folks!
\end{document}
