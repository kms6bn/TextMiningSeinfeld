{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Relationship between TV Comedy Scripts and Ratings\n",
    "## Katherine Schinkel and Marcus Rosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season                    int64\n",
      "Title                    object\n",
      "imdbID                   object\n",
      "Episode                   int64\n",
      "Released                 object\n",
      "imdbRating              float64\n",
      "No.overall                int64\n",
      "Directed by              object\n",
      "Written by               object\n",
      "Original air date        object\n",
      "Prod.code                object\n",
      "U.S.viewers_millions    float64\n",
      "ViewersNext             float64\n",
      "dtype: object\n",
      "Season      int64\n",
      "Episode     int64\n",
      "Text       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "friends = pd.read_csv(\"../data/friendsData.csv\")\n",
    "print(friends.dtypes)\n",
    "friends_text = pd.read_csv(\"../data/friendsScripts.csv\")\n",
    "print(friends_text.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season      int64\n",
      "Episode     int64\n",
      "Text       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "seinfeld_text = pd.read_csv(\"../data/seinfeldScripts.csv\", header=None)\n",
    "seinfeld_text.columns = [\"Season\", \"Episode\", \"Text\"]\n",
    "print(seinfeld_text.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season                    int64\n",
      "Released                 object\n",
      "Episode                   int64\n",
      "imdbRating              float64\n",
      "Title                    object\n",
      "No.overall               object\n",
      "No.inSeason               int64\n",
      "Directed                 object\n",
      "Written                  object\n",
      "Prod.Code                object\n",
      "U.S.viewers_millions    float64\n",
      "ViewersNext             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "seinfeld = pd.read_csv(\"../data/seinfeldData.csv\")\n",
    "print(seinfeld.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seinfeldAll = pd.merge(seinfeld, seinfeld_text, on=('Season', 'Episode'))\n",
    "friendsAll = pd.merge(friends, friends_text, on=('Season', 'Episode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#select target value\n",
    "#target = \"ViewersNext\"\n",
    "target = \"imdbRating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace missing values\n",
    "if target == \"ViewersNext\":\n",
    "    meanSeinfeld = sum(seinfeldAll['U.S.viewers_millions'])/len(seinfeldAll)\n",
    "    seinfeldAll['U.S.viewers_millions'] = seinfeldAll['U.S.viewers_millions'].replace(-1, meanSeinfeld)\n",
    "    seinfeldAll['ViewersNext'] = seinfeldAll['ViewersNext'].replace(-1, meanSeinfeld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop rows where ViewersNext is missing\n",
    "if target == \"ViewersNext\":\n",
    "    seinfeldAll = seinfeldAll.dropna()\n",
    "    friendsAll = friendsAll.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = [\"Text\",target]\n",
    "df1 = seinfeldAll[cols]\n",
    "df1.columns = ['Text', 'Target']\n",
    "\n",
    "df2 = friendsAll[cols]\n",
    "df2.columns = ['Text', 'Target']\n",
    "\n",
    "df = df1.append(df2, ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.columns = ['Text', 'Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get stop words\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import base\n",
    "\n",
    "class DenseTransformer(base.TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Try Stemming\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer(stop_words = en_stop).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(re.sub(r\"\\b\\d+\\b\", \"\", w.strip())) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instead of Stemming, try lemmatizing and tokenizing\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t.strip()) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training/testing\n",
    "doc_train, doc_test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = sum(doc_train['Target'])/len(doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand = np.random.randn(len(doc_test))\n",
    "meanDF = pd.DataFrame({\"mean\":mean,\"rand\":rand})[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.137210818588\n",
      "0.138231595107\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([\n",
    "                    #('vect', CountVectorizer(min_df=10, tokenizer=LemmaTokenizer())), #trigrams\n",
    "                    ('vect', CountVectorizer(analyzer=stemmed_words, min_df=10)), #stemming\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()),\n",
    "                     ('clf', LinearRegression())\n",
    "])\n",
    "text_clf.fit(doc_train[\"Text\"], doc_train['Target'])\n",
    "predicted_value = text_clf.predict(doc_test[\"Text\"])\n",
    "print(mean_squared_error(doc_test['Target'], predicted_value))\n",
    "print(mean_squared_error(doc_test['Target'], meanDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2509\n"
     ]
    }
   ],
   "source": [
    "print(len(text_clf.named_steps['clf'].coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.057927285023036117"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t-test\n",
    "(mean_squared_error(doc_test['Target'], predicted_value) - mean_squared_error(doc_test['Target'], meanDF)) / (np.std(predicted_value)/np.sqrt(len(predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.5</td>\n",
       "      <td>8.588365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.449487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>8.802441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.945472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.1</td>\n",
       "      <td>8.707590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.629688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.570380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.7</td>\n",
       "      <td>8.501490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.466440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.5</td>\n",
       "      <td>8.489548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.2</td>\n",
       "      <td>8.458604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.6</td>\n",
       "      <td>8.257534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.605678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.5</td>\n",
       "      <td>8.648158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.5</td>\n",
       "      <td>8.309872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.425400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.2</td>\n",
       "      <td>8.565397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.599178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.749587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.683251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.351656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.488340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.6</td>\n",
       "      <td>8.453580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.462850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.849119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.2</td>\n",
       "      <td>8.172949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.781094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.507435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9.2</td>\n",
       "      <td>8.906990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.790743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>9.1</td>\n",
       "      <td>8.565463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>8.6</td>\n",
       "      <td>8.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>8.6</td>\n",
       "      <td>8.621844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.699589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.293679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.763445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>8.2</td>\n",
       "      <td>8.098162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>9.5</td>\n",
       "      <td>8.704863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.529959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.390398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>9.1</td>\n",
       "      <td>8.708755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.535859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8.2</td>\n",
       "      <td>8.362251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.469840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>8.7</td>\n",
       "      <td>8.409646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.479579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.234989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>8.7</td>\n",
       "      <td>8.726091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.617143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.652869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>9.1</td>\n",
       "      <td>8.255559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.555075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>8.6</td>\n",
       "      <td>8.264774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.468059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>7.6</td>\n",
       "      <td>8.467396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>8.8</td>\n",
       "      <td>8.756617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>8.4</td>\n",
       "      <td>8.448610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.649253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>8.3</td>\n",
       "      <td>8.551775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1\n",
       "0    8.5  8.588365\n",
       "1    8.4  8.449487\n",
       "2    8.5  8.802441\n",
       "3    8.3  8.945472\n",
       "4    8.1  8.707590\n",
       "5    8.3  8.629688\n",
       "6    9.0  8.570380\n",
       "7    8.7  8.501490\n",
       "8    8.8  8.466440\n",
       "9    8.5  8.489548\n",
       "10   8.2  8.458604\n",
       "11   8.6  8.257534\n",
       "12   8.3  8.605678\n",
       "13   8.5  8.648158\n",
       "14   8.5  8.309872\n",
       "15   8.3  8.425400\n",
       "16   8.2  8.565397\n",
       "17   8.4  8.599178\n",
       "18   8.9  8.749587\n",
       "19   8.3  8.683251\n",
       "20   8.4  8.351656\n",
       "21   8.3  8.488340\n",
       "22   8.6  8.453580\n",
       "23   9.0  8.462850\n",
       "24   9.0  8.849119\n",
       "25   8.2  8.172949\n",
       "26   8.9  8.781094\n",
       "27   8.3  8.507435\n",
       "28   9.2  8.906990\n",
       "29   8.9  8.790743\n",
       "..   ...       ...\n",
       "85   9.1  8.565463\n",
       "86   8.6  8.600000\n",
       "87   8.6  8.621844\n",
       "88   8.9  8.699589\n",
       "89   8.3  8.293679\n",
       "90   8.9  8.763445\n",
       "91   8.2  8.098162\n",
       "92   9.5  8.704863\n",
       "93   8.4  8.529959\n",
       "94   8.3  8.390398\n",
       "95   9.1  8.708755\n",
       "96   8.3  8.535859\n",
       "97   8.2  8.362251\n",
       "98   8.3  8.469840\n",
       "99   8.7  8.409646\n",
       "100  8.8  8.479579\n",
       "101  8.4  8.234989\n",
       "102  8.7  8.726091\n",
       "103  8.4  8.617143\n",
       "104  8.8  8.652869\n",
       "105  9.1  8.255559\n",
       "106  8.4  8.555075\n",
       "107  8.6  8.264774\n",
       "108  8.8  8.800000\n",
       "109  8.4  8.468059\n",
       "110  7.6  8.467396\n",
       "111  8.8  8.756617\n",
       "112  8.4  8.448610\n",
       "113  8.9  8.649253\n",
       "114  8.3  8.551775\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at results\n",
    "pd.concat([pd.DataFrame(doc_test['Target'].values), pd.DataFrame(predicted_value)], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelmse = []\n",
    "meanmse = []\n",
    "kf = KFold(n=len(df), n_folds=10, shuffle=True,random_state=None)\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = df.Text[train_index], df.Text[test_index]\n",
    "    y_train, y_test = df.Target[train_index], df.Target[test_index]\n",
    "    text_clf = Pipeline([\n",
    "            #('vect', CountVectorizer(min_df=10, tokenizer=LemmaTokenizer())), #trigrams\n",
    "            ('vect', CountVectorizer(analyzer=stemmed_words,min_df=20)), #stemming\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('to_dense', DenseTransformer()),\n",
    "            ('clf', LinearRegression())\n",
    "    ])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    predicted_value = text_clf.predict(X_test)\n",
    "    mean_kf = sum(y_train)/len(y_train)\n",
    "    rand_kf = np.random.randn(len(y_test))\n",
    "    meanDF_kf = pd.DataFrame({\"mean\":mean_kf,\"rand\":rand_kf})[\"mean\"]\n",
    "    meanmse.append(mean_squared_error(y_test, meanDF_kf))\n",
    "    modelmse.append(mean_squared_error(y_test, predicted_value))\n",
    "    #print(\"Model mse: \",mean_squared_error(y_test, predicted_value))\n",
    "    #print(\"Mean mse : \",mean_squared_error(y_test, meanDF_kf))\n",
    "    #print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.132609319805\n",
      "0.173900420709\n",
      "-2.04982337819\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(meanmse))\n",
    "print(np.mean(modelmse))\n",
    "print((np.mean(meanmse)-np.mean(modelmse))/np.sqrt((np.std(meanmse)**2)/10+(np.std(modelmse)**2)/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.138534269016\n",
      "0.138231595107\n"
     ]
    }
   ],
   "source": [
    "#try topic modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "text_clf = Pipeline([\n",
    "                    #('vect', CountVectorizer(min_df=5, tokenizer=LemmaTokenizer())), #trigrams\n",
    "                    ('vect', CountVectorizer(analyzer=stemmed_words, min_df=10)), #stemming\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LatentDirichletAllocation(n_topics=40)),\n",
    "                    ('clf', LinearRegression())\n",
    "])\n",
    "text_clf.fit(doc_train[\"Text\"], doc_train['Target'])\n",
    "predicted_value = text_clf.predict(doc_test[\"Text\"])\n",
    "print(mean_squared_error(doc_test['Target'], predicted_value))\n",
    "print(mean_squared_error(doc_test['Target'], meanDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -26.96848238  -43.67698889  -39.76237612  -54.42685664  -25.23771934\n",
      "  -46.00232837 -110.25023773  -44.10487377  -30.80258886  -40.77366654]\n",
      "Mean Squared Error: -46.20 (+/- 37.86)\n"
     ]
    }
   ],
   "source": [
    "# run cross validation\n",
    "scores = cross_val_score(text_clf, df[\"Text\"], df[\"Target\"], cv=10, scoring=\"mean_squared_error\", n_jobs=-1)\n",
    "print(scores)\n",
    "print(\"Mean Squared Error: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 1.645))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelmse = []\n",
    "meanmse = []\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(n=len(df), n_folds=10, shuffle=False,random_state=None)\n",
    "for train_index, test_index in kf:\n",
    "    X_train, X_test = df.Text[train_index], df.Text[test_index]\n",
    "    y_train, y_test = df.Target[train_index], df.Target[test_index]\n",
    "    text_clf = Pipeline([\n",
    "                    #('vect', CountVectorizer(min_df=5, tokenizer=LemmaTokenizer())), #trigrams\n",
    "                    ('vect', CountVectorizer(analyzer=stemmed_words, min_df=10)), #stemming\n",
    "                    ('to_dense', DenseTransformer()),\n",
    "                    ('lda', LatentDirichletAllocation(n_topics=40)),\n",
    "                    ('clf', LinearRegression())\n",
    "    ])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    predicted_value = text_clf.predict(X_test)\n",
    "    mean_kf = sum(y_train)/len(y_train)\n",
    "    rand_kf = np.random.randn(len(y_test))\n",
    "    meanDF_kf = pd.DataFrame({\"mean\":mean_kf,\"rand\":rand_kf})[\"mean\"]\n",
    "    meanmse.append(mean_squared_error(y_test, meanDF_kf))\n",
    "    modelmse.append(mean_squared_error(y_test, predicted_value))\n",
    "    #print(\"Model mse: \",mean_squared_error(y_test, predicted_value))\n",
    "    #print(\"Mean mse : \",mean_squared_error(y_test, meanDF_kf))\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.132985964972\n",
      "0.132297672078\n",
      "0.0622007153507\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(meanmse))\n",
    "print(np.mean(modelmse))\n",
    "print((np.mean(meanmse)-np.mean(modelmse))/np.sqrt((np.std(meanmse)**2)/10+(np.std(modelmse)**2)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "oh just know yeah well like joey uh monica get hey see look go gonna dont want re right im\n",
      "Topic #1:\n",
      "yeah re just go like hey know look oh don can think get uh well want realli ll jerri gonna\n",
      "Topic #2:\n",
      "label oh super newman bowl ticket yeah well know go just get gift like hey can lane upstair look don\n",
      "Topic #3:\n",
      "pig bone maid spare know oh babi just look like don well hey yeah bra squar get sniff suck re\n",
      "Topic #4:\n",
      "take kept split upstair beyond particular bow silenc silent solv frantic bean subject spare interview romant estell just wh red\n",
      "Topic #5:\n",
      "oh well yeah know don go re hey get right see look just jerri like think can gonna georg uh\n",
      "Topic #6:\n",
      "oh know hey go yeah re like well look get can don joey just right one come uh got jerri\n",
      "Topic #7:\n",
      "right well know hey just ross like rachel oh can re look tell get think go guy realli yeah monica\n",
      "Topic #8:\n",
      "know just like re yeah okay oh go hey think can get don joey look uh one right want back\n",
      "Topic #9:\n",
      "know oh yeah go well just re get don can like right hey look think want see uh realli got\n",
      "Topic #10:\n",
      "well oh go just hey yeah get want know like jerri ticket win take gonna got suit good say dont\n",
      "Topic #11:\n",
      "know oh yeah just rachel okay well ross right get can joey go think re like realli monica got uh\n",
      "Topic #12:\n",
      "oh know hey ross joey well yeah just look okay im monica get like can rachel want go right got\n",
      "Topic #13:\n",
      "know re go get oh yeah well don just like look right hey okay uh ll can ross think joey\n",
      "Topic #14:\n",
      "oh know look im rachel chandler well just monica go okay hey yeah joey get see can dont phoeb like\n",
      "Topic #15:\n",
      "re well go oh yeah don like look know get joey hey uh okay think can just rachel chandler got\n",
      "Topic #16:\n",
      "oh get well know just like can go look think gonna rachel hey yeah im right don want jerri come\n",
      "Topic #17:\n",
      "oh go look yeah get know just like hey well monica right think can im okay uh come joey one\n",
      "Topic #18:\n",
      "oh yeah know get just well hey look like right gonna one see re ross think rachel mean can want\n",
      "Topic #19:\n",
      "paul jean wake alarm bastard bitch yeah oh know son get uh just race tub can come soak call got\n",
      "Topic #20:\n",
      "oh re don know well go yeah can get hey like just right okay ll look uh jerri joey got\n",
      "Topic #21:\n",
      "know re oh jerri well don go get georg hey ll yeah kramer can like just come right think elain\n",
      "Topic #22:\n",
      "get oh know yeah look well go hey im just gonna right take come re ross don ticket mean chandler\n",
      "Topic #23:\n",
      "im know oh go just rachel okay hey monica joey get ross dont right look yeah well phoeb gonna chandler\n",
      "Topic #24:\n",
      "know oh like yeah hey just go re right think can okay don look uh well joey get got talk\n",
      "Topic #25:\n",
      "oh go want know well can re yeah jerri get don like just ll hey right got georg think realli\n",
      "Topic #26:\n",
      "right want oh safe theme giddi like yeah joy legal gotta ralph soap easili land appear begin can anymor shall\n",
      "Topic #27:\n",
      "oh look like uh well can ross just think go yeah know monica see hey rachel guy back get right\n",
      "Topic #28:\n",
      "know oh just well get go monica chandler like uh look yeah re want joey hey right ross rachel don\n",
      "Topic #29:\n",
      "know well get yeah oh can right re don ross look okay hey like guy just see come uh gonna\n",
      "Topic #30:\n",
      "know rachel oh hey just get ross okay go joey monica chandler look well gonna right im yeah one uh\n",
      "Topic #31:\n",
      "well re oh uh just go tomorrow can get think like hey hi die grab kiss hello hundr know good\n",
      "Topic #32:\n",
      "oh okay re know well just like uh don yeah hey look right rachel go see chandler can monica ll\n",
      "Topic #33:\n",
      "oh just well yeah know monica go get hey don re can look uh ross joey rachel now key gonna\n",
      "Topic #34:\n",
      "oh okay rachel just joey go chandler know well right monica hey get can ross like yeah realli uh look\n",
      "Topic #35:\n",
      "oh don re get just like know can well yeah go right look gonna hey good want uh see okay\n",
      "Topic #36:\n",
      "oh chandler ross monica joey rachel know okay just look yeah well phoeb hey get like go gonna can right\n",
      "Topic #37:\n",
      "yeah oh well get look know monica go okay don just like hey right rachel guy ross re joey uh\n",
      "Topic #38:\n",
      "yeah go know like oh re hey can right look think get just don well okay want see rachel uh\n",
      "Topic #39:\n",
      "jerri card know check grandmoth oh fork cash dan finger yanke well knife like yeah flirt want don re save\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "tf_feature_names = text_clf.named_steps['vect'].get_feature_names()\n",
    "print_top_words(text_clf.named_steps['lda'], tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words, stop_words='english', ngram_range=(1,3))\n",
    "docs = vectorizer.fit_transform(seinfeldAll.Text)\n",
    "lda_model = lda.LDA(n_topics=40, n_iter=1500, random_state=1)\n",
    "lda_model = lda_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: brien lloyd paul jean murphi man wake hot rye game dial alarm gum bastard\n",
      "Topic 1: leo hello jerri uncl alright check card nana kiss open ask finger write doctor\n",
      "Topic 2: oh yeah well hey know just re get look got can see think one\n",
      "Topic 3: funni alright mickey doll david babu shoe beth gail gay man pasta tractor better\n",
      "Topic 4: show re idea russel noth nbc someth tv kramer charact come butler salsa right\n",
      "Topic 5: uh alright jerri okay ah em wanna smile hi point indic listen laugh shout\n",
      "Topic 6: jerri georg elain kramer now want back go come thing start take tri time\n",
      "Topic 7: morti move poni florida jack cadillac leo marisa coffe neil parent son condo cuban\n",
      "Topic 8: car park drive van move alright space pull god hit front citi key run\n",
      "Topic 9: drake toni big salad face handicap alright wheelchair huh ok butter rock newman jane\n",
      "Topic 10: bubbl test ha laugh hu boy ah sir moop bra babu donald steinbrenn iq\n",
      "Topic 11: hand door look uh open get take point turn walk coat box sit move\n",
      "Topic 12: uh bone tv guid alright new cat ok thank tuck gammi koko paper maid\n",
      "Topic 13: pen voic la play miss hello mr alright ball grace cosmo stay sleep tenni\n",
      "Topic 14: ya hey alright pari yea parent face morti cloth paus plan night uh bye\n",
      "Topic 15: soup yada one call shmoopi nazi jacki plane joke enter martin  mr four\n",
      "Topic 16: thousand dollar five hundr car apart hair fifti bottl nine littl ten four club\n",
      "Topic 17: ha ah ya name play um quiet wallet hu arm ball watch ok ahh\n",
      "Topic 18: peterman book mr card librari day newman top rememb new return ear year pictur\n",
      "Topic 19: georg father death nina paint beat danc brodi dip kiss lippman medicin blow anna\n",
      "Topic 20: jimmi clown move sex doctor blood tobi drink opera bozo assman red smell dot\n",
      "Topic 21: uh give susi leav stori festivus get mail peterman card birthday cough train bagel\n",
      "Topic 22: susan hey ha puddi cigar wed jacket ross candi bar movi bob twix deal\n",
      "Topic 23: dog sob seren chines cape donna signal chang dad jacket man father comput noreen\n",
      "Topic 24: maestro massag pam fruit hair fake newman chair bald  tuscani haircut cours mr\n",
      "Topic 25: arm ve yogurt doctor fat glass sniff eh point bra lloyd snif swim mattress\n",
      "Topic 26: coffe realli tabl woman  read book great alright thing eat alway boy like\n",
      "Topic 27: man babi pig mohel nake heart step jane hospit apolog lobster ya tonsil shower\n",
      "Topic 28: ll re go get take time back thing will money work cours hell turn\n",
      "Topic 29: key em apart statu ray give clean flea coincid phil food kramer rava pitt\n",
      "Topic 30: tim voight jon super hey alright label whatley song seinfeld check pop jerri bowl\n",
      "Topic 31: suit poppi ticket pie couch hand walk bania pizza think duck soup dress break\n",
      "Topic 32: go gonna okay kramer busi banker get now yo show dollar bank gotta doctor\n",
      "Topic 33: know go don like can say right just uh want come get re guy\n",
      "Topic 34: gonna re jerri marri come kramer wanna mean door sex bag woman kill morn\n",
      "Topic 35: nake around name fish cabl bike money even shirt soda hospit laundri dri stock\n",
      "Topic 36:  ok guy eat chicken food cake kramer skin mile towel borrow shave eh\n",
      "Topic 37: uh joe birthday nina wish mayo card newman wed coat friend massag man fur\n",
      "Topic 38: uh keith ticket  smell rochell ho vandelay guy babi communist race seat hernandez\n",
      "Topic 39: dog eacut date okay mr calzon rememb risk cook mentor three bet code frame\n"
     ]
    }
   ],
   "source": [
    "topic_word = lda_model.topic_word_ \n",
    "vocab = vectorizer.get_feature_names()\n",
    "n_top_words = 15\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
